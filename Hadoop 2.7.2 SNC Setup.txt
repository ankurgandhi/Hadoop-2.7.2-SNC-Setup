#Follow the below steps to configure Hadoop 2 Single Node Setup 

# Install Java 8
sudo apt-get install -y python-software-properties debconf-utils && sudo add-apt-repository -y ppa:webupd8team/java && sudo apt-get update && sudo apt-get install -y oracle-java8-installer

# Install Hadoop
wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz -P ~/Downloads
sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local
sudo mv /usr/local/hadoop-* /usr/local/hadoop

# Set Enviornment Variable
readlink -f $(which java)

cat >>$HOME/.bashrc <<EOL
# -- HADOOP ENVIRONMENT VARIABLES START -- #
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HOME=/usr/local/hadoop
export PATH=\$PATH:\$HADOOP_HOME/bin
export PATH=\$PATH:\$HADOOP_HOME/sbin
export PATH=\$PATH:/usr/local/hadoop/bin/
export HADOOP_MAPRED_HOME=\$HADOOP_HOME
export HADOOP_COMMON_HOME=\$HADOOP_HOME
export HADOOP_HDFS_HOME=\$HADOOP_HOME
export YARN_HOME=\$HADOOP_HOME
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export PDSH_RCMD_TYPE=ssh
# -- HADOOP ENVIRONMENT VARIABLES END -- #
EOL

exec bash

#Default User for hadoop directory should be ubuntu
sudo chown -R ubuntu:ubuntu /usr/local/hadoop


#Update hadoop-env.sh to set JAVA_HOME and HADOOP_LOG_DIR environment variable

sudo su -c 'echo export JAVA_HOME=/usr/lib/jvm/java-8-oracle >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh'

sudo su -c 'echo export HADOOP_LOG_DIR=/var/log/hadoop/ >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh'

sudo mkdir /var/log/hadoop/

sudo chown ubuntu:ubuntu -R /var/log/hadoop

# Disable SELINUX
 
sudo apt-get install selinux-utils

setenforce 0

cat /etc/selinux/config

SELINUX=disabled
SELINUXTYPE=targeted
SETLOCALDEFS=0

#Disable IPV6 

cat /proc/sys/net/ipv6/conf/all/disable_ipv6

sudo sysctl -p

sudo su -c 'cat >>/etc/sysctl.conf <<EOL
net.ipv6.conf.all.disable_ipv6 =1
net.ipv6.conf.default.disable_ipv6 =1
net.ipv6.conf.lo.disable_ipv6 =1
EOL'

#Disable FireWall iptables

sudo iptables -L -n -v 

sudo iptables-save > firewall.rules 

#The Uncomplicated Firewall or ufw is the configuration tool for iptables that comes by default on Ubuntu

sudo ufw status verbose 

sudo ufw status verbose


#Disabling Transparent Hugepage Compaction

#Red Hat/CentOS: /sys/kernel/mm/redhat_transparent_hugepage/defrag

#Ubuntu/Debian, OEL, SLES: /sys/kernel/mm/transparent_hugepage/defrag

cat /sys/kernel/mm/transparent_hugepage/defrag

sudo su -c 'cat >>/etc/rc.local <<EOL
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
  echo never > /sys/kernel/mm/transparent_hugepage/enabled
fi
if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
   echo never > /sys/kernel/mm/transparent_hugepage/defrag 
fi
exit 0
EOL'

sudo -i

source /etc/rc.local


# Set Swappiness

sudo sysctl -a | grep vm.swappiness

sudo su -c 'cat >>/etc/sysctl.conf <<EOL
'vm.swappiness=0'
EOL'


# Configure NTP 

timedatectl status
timedatectl list-timezones
timedatectl list-timezones | grep Asia/Kolkata
sudo timedatectl set-timezone Asia/Kolkata
timedatectl status
sudo ntpq -p
sudo apt-get install ntp -Y
timedatectl status

sudo nano /etc/ntp.conf


# Root Reserved Space

mkfs.ext4 -m 0 /dev/xvda1 ( filesystem is not suppose to be mounted)

sudo file -sL /dev/xvda1

lsblk

sudo tune2fs -m 0 /dev/xvda1


#Configure SSH Password less logins 

sudo su -c touch /home/ubuntu/.ssh/config; echo -e \ "Host *\n StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null" \ > ~/.ssh/config

echo -e  'y\n'| ssh-keygen -t rsa -P "" -f $HOME/.ssh/id_rsa

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

sudo service ssh restart

ssh localhost

#Update core-site.xml

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/core-site.xml

sudo su -c 'cat >> /usr/local/hadoop/etc/hadoop/core-site.xml <<EOL
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
EOL'


#Update hdfs-site.xml 

mkdir -p /usr/local/hadoop/data/hdfs/namenode
mkdir -p /usr/local/hadoop/data/hdfs/datanode

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/hdfs-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/hdfs-site.xml <<EOL
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property>
</configuration>
EOL'


#Update yarn-site.xml

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/yarn-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/yarn-site.xml <<EOL
<configuration>
<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
  </property>
</configuration>
EOL'



#Update mapred-site.xml

cp  /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/mapred-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/mapred-site.xml <<EOL
<configuration>
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value>localhost:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
EOL'


#Since above commands are executed using "sudo" , thus execute below command to set "ubuntu" as user for $HADOOP_HOME user.
sudo chown -R ubuntu:ubuntu $HADOOP_HOME



#Format Namenode and start Cluster Services

hdfs namenode -format

start-dfs.sh

$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver

start-yarn.sh

#check whether all services are started
jps

#Output of jps
5602 Jps
5347 JobHistoryServer
4980 DataNode
5191 SecondaryNameNode
5417 ResourceManager
4825 NameNode
5567 NodeManager

#Kudos Hadoop 2 on Single Node is successfully configured!!!
 Desktop version
